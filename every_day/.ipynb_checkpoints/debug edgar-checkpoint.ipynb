{
 "metadata": {
  "name": "",
  "signature": "sha256:3fe3eb448f9b6b6f44df2bbdb0af7514f90f6e8fe68d90d38656be2f189e1ed4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import Libraries\n",
      "from ftplib import FTP, all_errors\n",
      "from time import sleep\n",
      "from datetime import datetime\n",
      "import pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get list of directories (assumes a certain structure)\n",
      "def get_dir_list(ftp_class):\n",
      "    ls = []\n",
      "    dir_list = []\n",
      "    ftp_class.retrlines('MLSD', ls.append)\n",
      "    for entry in ls:\n",
      "        split = entry.split(\";\")\n",
      "        if split[2] == \"type=dir\":\n",
      "            dir_list.append(split[-1][1:])\n",
      "\n",
      "    return dir_list\n",
      "    \n",
      "#Download file to local directory; use switch to determine naming conventions\n",
      "#Returns the local filepath to be fed into the next function***Test this\n",
      "def download_local_file(filename, destination, switch, ftp_class):\n",
      "    \n",
      "    #If we want to append part of the directory name to the file\n",
      "    if switch == 2:\n",
      "        #Get the last two elements of the filepath\n",
      "        filepath = ftp_class.pwd()\n",
      "        \n",
      "        split = filepath.split('/')\n",
      "        extension = \"_\".join(split[-2:])\n",
      "\n",
      "        local_file = open(destination + extension + \"_\" + filename, 'w')\n",
      "        ftp_class.retrbinary(\"RETR \" + filename, local_file.write)\n",
      "        local_file.close()\n",
      "        \n",
      "        return destination + extension + \"_\" + filename\n",
      "    \n",
      "    elif switch == 1:\n",
      "\n",
      "        #local_file = open(destination + extension + \"_\" + filename, 'w')\n",
      "        binary_data = []\n",
      "        ftp_class.retrbinary(\"RETR \" + filename, binary_data.append)\n",
      "        #local_file.close()\n",
      "        \n",
      "        return binary_data\n",
      "    \n",
      "    elif switch == 3:\n",
      "                \n",
      "        split = filename.split('/')\n",
      "        name = destination + split[-2] + \"_\" + split[-1]\n",
      "\n",
      "        local_file = open(name, 'w')\n",
      "        ftp_class.retrbinary(\"RETR \" + filename, local_file.write)\n",
      "        local_file.close()\n",
      "        \n",
      "        return name\n",
      "    #if not...\n",
      "    else:\n",
      "        \n",
      "        local_file = open(destination + filename, 'w')\n",
      "        ftp_class.retrbinary(\"RETR \" + filename, local_file.write)\n",
      "        local_file.close()\n",
      "        \n",
      "        return destination + extension + \"_\" + filename\n",
      "\n",
      "#This function compares whether it is 6 am yet or not\n",
      "def after_six_am():\n",
      "    now = datetime.now().time()\n",
      "    stop_time=datetime.strptime(\"06:00:00\",\"%H:%M:%S\").time()\n",
      "    nine_pm = datetime.strptime(\"21:00:00\",\"%H:%M:%S\").time()\n",
      "    return now > stop_time and now < nine_pm\n",
      "    \n",
      "#Login function\n",
      "def login():\n",
      "    try:\n",
      "        ftp = FTP(\"ftp.sec.gov\")\n",
      "        ftp.login()\n",
      "        ftp.pwd()\n",
      "        return ftp\n",
      "        \n",
      "    except:\n",
      "        sleep(301)\n",
      "        print \"not working\"\n",
      "        login()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ftp = FTP(\"ftp.sec.gov\")\n",
      "ftp.login()\n",
      "ftp.pwd()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "'/'"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    #Define paths as variables so they can be easily changed later\n",
      "    ####Still need to put in something to catch any disconnects and re-login#####\n",
      "    metadata_output_file = \"C:\\\\Users\\\\zacharybeaver\\\\Documents\\\\Github_stuffs\\\\web_scraping\\\\edgar\\\\debug\\\\metadata.txt\"\n",
      "    bad_list_output_file = \"C:\\\\Users\\\\zacharybeaver\\\\Documents\\\\Github_stuffs\\\\web_scraping\\\\edgar\\\\debug\\\\bad_list.txt\"\n",
      "    dl_doc_destination = \"C:\\\\Users\\\\zacharybeaver\\\\Documents\\\\Github_stuffs\\\\web_scraping\\\\edgar\\\\debug\\\\documents\\\\\"\n",
      "    index_pickle_file = \"C:\\\\Users\\\\zacharybeaver\\\\Documents\\\\Github_stuffs\\\\web_scraping\\\\edgar\\\\debug\\\\indices\\\\indexes_left.pickle\"\n",
      "    metadata_pickle_file = \"C:\\\\Users\\\\zacharybeaver\\\\Documents\\\\Github_stuffs\\\\web_scraping\\\\edgar\\\\debug\\\\indices\\\\metadata_left.pickle\"\n",
      "    \n",
      "    \n",
      "    #Load the pickle files and start the process    \n",
      "    indices_list = pickle.load(open(index_pickle_file, \"rb\"))\n",
      "    print \"indices loaded\"\n",
      "    metadata_list = pickle.load(open(metadata_pickle_file, \"rb\"))\n",
      "    print \"metadata loaded too!\"\n",
      "    \n",
      "    #Login to the Edgar FTP\n",
      "    ftp = login()\n",
      "    print \"successful login\"\n",
      "                \n",
      "        \n",
      "    #Open a metadata file to begin collecting all the metadata\n",
      "    meta_file = open(metadata_output_file, \"a\")\n",
      "    bad_list = open(bad_list_output_file, \"a\")\n",
      "    \n",
      "    #Loop through each item in the index list\n",
      "    for i in range(5):\n",
      "        try:\n",
      "            \n",
      "            #Get the document of interest and write it's resulting filepath and metadata to the metafile\n",
      "            meta_file.write(metadata_list[i] + \"|\" + download_local_file(indices_list[i], dl_doc_destination, 3, ftp) + \"\\n\")\n",
      "            print \"got\", i\n",
      "        \n",
      "        #If there's a connection error, write the file path to the bad_list and reconnect\n",
      "        except all_errors:\n",
      "            ftp = login()\n",
      "            bad_list.write(indices_list[i] + \"\\n\")\n",
      "            \n",
      "        except:\n",
      "            bad_list.write(indices_list[i] + \"\\n\")\n",
      "    \n",
      "                \n",
      "    with open(index_pickle_file, \"wb\") as f:\n",
      "        #Only dump all the indices that haven't been traversed\n",
      "        pickle.dump(indices_list[i + 1:], f)\n",
      "        \n",
      "    with open(metadata_pickle_file, \"wb\") as f:\n",
      "        #only dump indices that haven't been traversed\n",
      "        pickle.dump(metadata_list[i + 1:], f)\n",
      "    \n",
      "    meta_file.close()\n",
      "    bad_list.close()\n",
      "\n",
      "#check metafile\n",
      "#check new pickle after load\n",
      "#check that files downloaded\n",
      "#check bad list\n",
      "#Need to note when the last index day was pulled!!!\n",
      "#change the file paths to server ones\n",
      "    print i\n",
      "    print indices_list[0]\n",
      "    print indices_list[i]\n",
      "    print indices_list[i + 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "indices loaded\n",
        "metadata loaded too!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "successful login"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "got"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0\n",
        "got"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n",
        "got"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "got"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "got"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/edgar/data/66740/0000066740-94-000015.txt\n",
        "/edgar/data/99193/0000950131-93-000153.txt\n",
        "/edgar/data/909465/0001021408-01-503754.txt\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "indices_list = list()\n",
      "metadata_list = list()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}