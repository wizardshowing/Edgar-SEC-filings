{
 "metadata": {
  "name": "",
  "signature": "sha256:6b906bb1a98d3f3819024fe7f3cc96b7878610a2a511c13bedf65ee37316088d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###Need to think about how to check that all the files got downloaded --> create a pickle of unobtainable files/filepaths;\n",
      "#Check to see if anythink is in that pickle file and try those paths first; if they work, go on; if not, write them out to pickle again\n",
      "\n",
      "#Import Libraries\n",
      "from ftplib import FTP, all_errors\n",
      "from time import sleep\n",
      "from datetime import datetime\n",
      "import pickle\n",
      "import sunburnt\n",
      "import re\n",
      "from time import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This function compares whether it is 6 am yet or not\n",
      "def after_six_am():\n",
      "    now = datetime.now().time()\n",
      "    stop_time=datetime.strptime(\"06:00:00\",\"%H:%M:%S\").time()\n",
      "    nine_pm = datetime.strptime(\"20:45:00\",\"%H:%M:%S\").time()\n",
      "    return now > stop_time and now < nine_pm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to extract header and body from EDGAR file\n",
      "def get_header_body(edgar_filepath):\n",
      "    \n",
      "    #Open the file\n",
      "    with open(edgar_filepath, \"r\") as f:\n",
      "        preprocess = list(f)\n",
      "        \n",
      "    #Get rid of the excess at the top and bottom\n",
      "    edgar_string = \"\".join(preprocess[10:-1])\n",
      "    \n",
      "    #Get the header\n",
      "    header_list = re.split(\"<\\/?SEC-HEADER>\",edgar_string)\n",
      "    header = header_list[1]\n",
      "    \n",
      "    #Get the body\n",
      "    edgar_list = re.split(\"<\\/?TEXT>\",edgar_string)\n",
      "    body = \"\"\n",
      "    for i in range(len(edgar_list)):\n",
      "        if i%2==1:\n",
      "            body = body + edgar_list[i]\n",
      "            \n",
      "    #Return the header and body\n",
      "    return header, body"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Login function\n",
      "def login():\n",
      "    try:\n",
      "        ftp = FTP(\"ftp.sec.gov\")\n",
      "        ftp.login()\n",
      "        ftp.pwd()\n",
      "        return ftp\n",
      "        \n",
      "    except:\n",
      "            sleep(20)\n",
      "            login()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Download file to local directory; use switch to determine naming conventions\n",
      "#Returns the local filepath to be fed into the next function***Test this\n",
      "def download_local_file(filename, destination, switch, ftp_class):\n",
      "    \n",
      "    #If we just want to download the file with its original name to the destination\n",
      "    if switch == 2:\n",
      "       \n",
      "        local_file = open(destination + filename, 'w')\n",
      "        ftp_class.retrbinary(\"RETR \" + filename, local_file.write)\n",
      "        local_file.close()\n",
      "        \n",
      "        return destination + filename\n",
      "    \n",
      "    #Return binary data to the python program\n",
      "    elif switch == 1:\n",
      "\n",
      "        #local_file = open(destination + extension + \"_\" + filename, 'w')\n",
      "        binary_data = []\n",
      "        ftp_class.retrbinary(\"RETR \" + filename, binary_data.append)\n",
      "        #local_file.close()\n",
      "        \n",
      "        return binary_data\n",
      "    \n",
      "    #Download the file and append part of the original filepath to the name. Returns the local destination\n",
      "    elif switch == 3:\n",
      "                \n",
      "        split = filename.split('/')\n",
      "        name = destination + split[-2] + \"_\" + split[-1]\n",
      "\n",
      "        local_file = open(name, 'w')\n",
      "        ftp_class.retrbinary(\"RETR \" + filename, local_file.write)\n",
      "        local_file.close()\n",
      "        \n",
      "        return name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_file_paths_daily(indexed_file):\n",
      "    \n",
      "    with open(indexed_file, \"r\") as index_file:\n",
      "        index_list = list(index_file)\n",
      "    \n",
      "    #Find indices for splitting fields (since their fixed-width)\n",
      "    index_list = index_list[11:]\n",
      "    company_name_i = 62\n",
      "    form_type_i = 74\n",
      "    cik_i = 86\n",
      "    date_filed_i = 98\n",
      "    file_name_i = len(index_list[0])\n",
      "\n",
      "    cleaned_indices = []\n",
      "    file_paths = []\n",
      "    \n",
      "    for index in index_list:\n",
      "        metadata = []\n",
      "        metadata.append(index[:company_name_i].rstrip())\n",
      "        metadata.append(index[company_name_i:form_type_i].rstrip())\n",
      "        metadata.append(index[form_type_i:cik_i].rstrip())\n",
      "        metadata.append(index[cik_i:date_filed_i].rstrip())\n",
      "        file_path = index[date_filed_i:file_name_i].rstrip()\n",
      "        metadata.append(file_path)\n",
      "        file_paths.append(\"/\" + file_path)\n",
      "        cleaned_indices.append(metadata)\n",
      "        \n",
      "    return file_paths, cleaned_indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bad = []\n",
      "pickle.dump(bad, open(\"already_obtained.pickle\", \"wb\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_idx_list(ftp_class):\n",
      "    ls = []\n",
      "    file_list = []\n",
      "    ftp_class.retrlines('MLSD', ls.append)\n",
      "    for entry in ls:\n",
      "        split = entry.split(\";\")\n",
      "        try:\n",
      "            if split[3] == \"type=file\" and split[-1][:8] == \" company\" and split[-1][-3:] == \"idx\":\n",
      "                file_list.append(split[-1][1:])\n",
      "        except IndexError:\n",
      "            pass\n",
      "    return file_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "obtained_filepath = \"already_obtained.pickle\"\n",
      "update_destination = \""\n",
      "metadata_output_file = \"metadata.txt\"\n",
      "bad_list_output_file = \"bad_list.txt\"\n",
      "\n",
      "ftp = login()\n",
      "\n",
      "#Navigate to the daily_index\n",
      "ftp.cwd(\"/edgar/daily-index/\")\n",
      "\n",
      "#Read in a list of files already obtained\n",
      "already_obtained = pickle.load(open(obtained_filepath, \"rb\"))\n",
      "\n",
      "#obtain a directory list of the file names and remove those not ending in \".idx\" and starting with \"company\"\n",
      "file_list = get_idx_list(ftp)\n",
      "\n",
      "\n",
      "#Check to see if the already_obtained list elements are in the directory list\n",
      "removal_indices = []\n",
      "for i in range(len(already_obtained)):\n",
      "    \n",
      "    #if so, pass\n",
      "    try:\n",
      "        file_list.index(already_obtained[i])\n",
      "    \n",
      "    #else, store the index to later use to remove the item\n",
      "    except ValueError:\n",
      "        removal_indices.append(i)\n",
      "\n",
      "#remove item from already obtained...this stops the list from growing infinitely\n",
      "for index in sorted(removal_indices, reverse=True):\n",
      "    del already_obtained[index] \n",
      "\n",
      "#Open a metadata file to begin collecting all the metadata\n",
      "###Fix this...I don't think we want to append...maybe make a new file each day?###\n",
      "meta_file = open(metadata_output_file, \"a\")\n",
      "bad_list = open(bad_list_output_file, \"a\")\n",
      "\n",
      "#Loop through each file name in the new list and check to see if it's in the already_obtained\n",
      "for i in range(len(file_list)):\n",
      "    \n",
      "    #If it is in the already downloaded list, pass\n",
      "    try:\n",
      "        already_obtained.index(file_list[i])\n",
      "    \n",
      "    #Else, download it and add its name to the \"already downloaded\" list\n",
      "    except ValueError:\n",
      "        \n",
      "        #Update the already_obtained list\n",
      "        already_obtained.append(file_list[i])\n",
      "        \n",
      "        #download the index files\n",
      "        local_filepath = download_local_file(file_list[i], update_destination, 2, ftp)\n",
      "\n",
      "        #Get the filepaths and metadata\n",
      "        paths, metadata = get_file_paths(local_filepath)\n",
      "        \n",
      "        #Loop through these files and download the new documents\n",
      "        for i in range(len(paths)):\n",
      "            \n",
      "            #Try to download the document and write out its metadata to file\n",
      "            try:\n",
      "\n",
      "                #Get the document of interest\n",
      "                meta_file.write(metadata[i][0] + \"|\" + metadata[i][1] + \"|\" + metadata[i][2] + \"|\" + metadata[i][3] + \"|\" + download_local_file(paths[i], dl_doc_destination, 3, ftp) + \"\\n\")\n",
      "            \n",
      "            #If connection error, retry connection\n",
      "            except all_errors:\n",
      "                login()\n",
      "                bad_list.write(paths[i] + \"\\n\")\n",
      "            \n",
      "            #anything else, write to bad_list\n",
      "            except:\n",
      "                bad_list.write(paths[i] + \"\\n\")\n",
      "        \n",
      "bad_list.close()\n",
      "meta_file.close()\n",
      "with open(obtained_filepath, \"wb\") as f:\n",
      "    #Only dump all the indices that haven't been traversed\n",
      "    pickle.dump(already_obtained, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Read from the metafile and load into Solr\n",
      "metafile = \"\"\n",
      "\n",
      "#Read the metafile into a list\n",
      "with open(metafile,\"r\") as f:\n",
      "        metalist = list(f)\n",
      "        \n",
      "final_list = []\n",
      "\n",
      "#split the metalist into component parts\n",
      "for line in metalist:\n",
      "    if line[-1] == \"\\n\":\n",
      "        new_line = line[:-1]\n",
      "    else:\n",
      "        new_line = line\n",
      "    new_metalist = new_line.split(\"|\")\n",
      "    final_list.append(new_metalist)\n",
      "\n",
      "# Create connection to Solr\n",
      "solr_interface = sunburnt.SolrInterface(\"")\n",
      "\n",
      "#Load the items into Solr\n",
      "for item in final_list:\n",
      "    company = item[0]\n",
      "    form = item[1]\n",
      "    cik = item[2]\n",
      "    date = item[3]\n",
      "    path = item[4]\n",
      "    header, body = get_header_body(path) \n",
      "    document = {\n",
      "    \"cik\":cik,\n",
      "    \"company\":company,\n",
      "    \"date\":date,\n",
      "    \"header\":header,\n",
      "    \"body\":body,\n",
      "    \"form\":form,\n",
      "    \"path\":path\n",
      "    }\n",
      "    solr_interface.add(document)\n",
      "\n",
      "solr_interface.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
